{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bc524e",
   "metadata": {},
   "source": [
    "# Networkx ATLAS KG construction and RAG example\n",
    "This notebook demonstrates the full streamlined process of creating a knowledge graph (KG) using the atlas-rag package and performing retrieval-augmented generation (RAG) with our created RAG methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a035b3",
   "metadata": {},
   "source": [
    "## ATLAS KG construction\n",
    "It is suggested to use local hf model to run the KG construction code, as llm api service provider use optimized, lightweight models to reduce costs, which may sacrifice performance, and hence hard to have guaranteed performance. (for example from fp16 to bf16 etc.)\n",
    "\n",
    "ATLAS KG construction consist of 5 steps:\n",
    "- Triples Json Generation (Base KG Json)\n",
    "- Convert Triples Json to Triples csv\n",
    "- Conceptualize Entity in Triples csv\n",
    "- Merge Concept CSV to Triples CSV\n",
    "- Convert CSV to graphml for networkx to perform rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag import TripleGenerator, KnowledgeGraphExtractor, ProcessingConfig\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline\n",
    "# client = OpenAI(api_key='<your_api_key>',base_url=\"<your_api_base_url>\") \n",
    "# model_name = \"meta-llama/llama-3.1-8b-instruct\"\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "client = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "keyword = 'Dulce'\n",
    "output_directory = f'import/{keyword}'\n",
    "triple_generator = TripleGenerator(client, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37353f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extraction_config = ProcessingConfig(\n",
    "      model_path=model_name,\n",
    "      data_directory=\"tests\",\n",
    "      filename_pattern=keyword,\n",
    "      batch_size=2,\n",
    "      output_directory=f\"{output_directory}\",\n",
    ")\n",
    "kg_extractor = KnowledgeGraphExtractor(model=triple_generator, config=kg_extraction_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c248a3",
   "metadata": {},
   "source": [
    "### Triples Generation (with OpenAI Package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct entity&event graph\n",
    "kg_extractor.run_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Triples Json to CSV\n",
    "kg_extractor.convert_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335211ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept Generation\n",
    "kg_extractor.generate_concept_csv_temp(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5823e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extractor.create_concept_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480f15",
   "metadata": {},
   "source": [
    "## Choice 1: convert to graphml for networkx rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv to graphml for networkx\n",
    "kg_extractor.convert_to_graphml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a428",
   "metadata": {},
   "source": [
    "## Choice 2: Convert to neo4j dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numeric id to the csv so that we can use vector indices\n",
    "kg_extractor.add_numeric_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607bbc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extractor.compute_embedding() # default encoder_model_name=\"all-MiniLM-L12-v2\"\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"all-MiniLM-L12-v2\")\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"nvidia/NV-Embed-v2\")\n",
    "kg_extractor.create_faiss_index() # default index_type=\"HNSW,Flat\"\n",
    "\n",
    "# kg_extractor.create_faiss_index(index_type=\"HNSW,Flat\")\n",
    "# kg_extractor.create_faiss_index(index_type=\"IVF65536_HNSW32,Flat\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cd18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c8f197d",
   "metadata": {},
   "source": [
    "## ATLAS RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b957c1",
   "metadata": {},
   "source": [
    "In order to perform RAG, one need to first create embeddings & faiss index for constructed KG\n",
    "\n",
    "[There maybe performance difference in using AutoModel and Sentence Transformer for NV-Ebmed-v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859bf50",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 4\n",
      "GPU 0: NVIDIA L20\n",
      "GPU 1: NVIDIA L20\n",
      "GPU 2: NVIDIA L20\n",
      "GPU 3: NVIDIA L20\n"
     ]
    }
   ],
>>>>>>> 9148661 (refactor: neo4j_api for large kg rag)
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3,4'  # Set to the GPU you want to use, or '0' for the first GPU\n",
    "import torch\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"number of GPUs available:\", torch.cuda.device_count())\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d1aa8",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/httsangaj/miniconda3/envs/faiss-gpu/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/httsangaj/projects/AutoSchemaKG/atlas_rag/billion/prompt_template.py:27: SyntaxWarning: invalid escape sequence '\\('\n",
      "  \"content\": \"[[ ## question ## ]]\\nSolve \\(x^2 - 5x + 6 = 0\\).\"\n",
      "/home/httsangaj/projects/AutoSchemaKG/atlas_rag/billion/prompt_template.py:167: SyntaxWarning: invalid escape sequence '\\('\n",
      "  \"content\": \"\"\"[[ ## question ## ]]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.30s/it]\n"
     ]
    }
   ],
>>>>>>> 9148661 (refactor: neo4j_api for large kg rag)
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.retriever import NvEmbed\n",
    "from transformers import AutoModel\n",
    "# Load the SentenceTransformer model\n",
    "encoder_model_name = \"nvidia/NV-Embed-v2\"\n",
    "# sentence_model = SentenceTransformer(encoder_model_name, trust_remote_code=True, model_kwargs={'device_map': \"auto\"})\n",
    "# sentence_model.max_seq_length = 32768\n",
    "# sentence_model.tokenizer.padding_side=\"right\"\n",
    "sentence_model = AutoModel.from_pretrained(encoder_model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "sentence_encoder = NvEmbed(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a106e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from atlas_rag.reader import LLMGenerator\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# reader_model_name = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "reader_model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "client = OpenAI(\n",
    "  # base_url=\"https://openrouter.ai/api/v1\",\n",
    "  # api_key=config['settings']['OPENROUTER_API_KEY'],\n",
    "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "  api_key=config['settings']['DEEPINFRA_API_KEY'],\n",
    ")\n",
    "llm_generator = LLMGenerator(client=client, model_name=reader_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ba40a",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using encoder model: NV-Embed-v2\n",
      "Loading graph from /data/httsangaj/atomic-rag/8b/kg_graphml/musique_graph.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262675/262675 [00:00<00:00, 2455275.98it/s]\n",
      "100%|██████████| 262675/262675 [00:00<00:00, 1809056.30it/s]\n",
      "955769it [00:00, 3788499.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings already computed.\n",
      "Node and edge embeddings not found, computing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding nodes:   0%|          | 0/3923 [00:00<?, ?it/s]/home/httsangaj/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/c50d55f43bde7e6a18e0eaa15a62fd63a930f1a1/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n",
      "Encoding nodes: 100%|██████████| 3923/3923 [3:35:00<00:00,  3.29s/it]  \n",
      "Encoding edges:  68%|██████▊   | 3584/5259 [3:36:17<1:38:39,  3.53s/it]"
     ]
    }
   ],
>>>>>>> 9148661 (refactor: neo4j_api for large kg rag)
   "source": [
    "from atlas_rag import create_embeddings_and_index\n",
    "keyword = 'musique'\n",
    "working_directory = f'/data/httsangaj/atomic-rag/8b'\n",
    "data = create_embeddings_and_index(\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    model_name = 'nvidia/NV-Embed-v2',\n",
    "    working_directory=working_directory,\n",
    "    keyword=keyword,\n",
    "    include_concept=True,\n",
    "    include_events=True,\n",
    "    normalize_embeddings= True,\n",
    "    text_batch_size=64,\n",
    "    node_and_edge_batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag.evaluation import BenchMarkConfig\n",
    "benchmark_config = BenchMarkConfig(\n",
    "    dataset_name= 'musique',\n",
    "    question_file= \"benchmark_data/musique.json\",\n",
    "    include_concept=True,\n",
    "    include_events=True,\n",
    "    reader_model_name=reader_model_name,\n",
    "    encoder_model_name=encoder_model_name,\n",
    "    number_of_samples=-1, # -1 for all samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag import setup_logger\n",
    "logger = setup_logger(benchmark_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize desired RAG method for benchmarking\n",
    "from atlas_rag.retriever import HippoRAG2Retriever\n",
    "hipporag2_retriever = HippoRAG2Retriever(\n",
    "    llm_generator=llm_generator,\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    data = data,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d653a",
   "metadata": {},
   "source": [
    "## Investigation for reason to perfomance difference:\n",
    "- Version difference for cuda?\n",
    "- Version difference for huggingface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b515ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start benchmarking\n",
    "from atlas_rag.evaluation import RAGBenchmark\n",
    "benchmark = RAGBenchmark(config=benchmark_config, logger=logger)\n",
    "benchmark.run([hipporag2_retriever], llm_generator=llm_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca8216",
   "metadata": {},
   "source": [
    "## Billion Level KG RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a5f16",
   "metadata": {},
   "source": [
    "from atlas_rag.billion import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoschemakg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
