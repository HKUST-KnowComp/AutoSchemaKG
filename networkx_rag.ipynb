{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bc524e",
   "metadata": {},
   "source": [
    "# Networkx ATLAS KG construction and RAG example\n",
    "This notebook demonstrates the full streamlined process of creating a knowledge graph (KG) using the atlas-rag package and performing retrieval-augmented generation (RAG) with our created RAG methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a035b3",
   "metadata": {},
   "source": [
    "## ATLAS KG construction\n",
    "It is suggested to use local hf model to run the KG construction code, as llm api service provider use optimized, lightweight models to reduce costs, which may sacrifice performance, and hence hard to have guaranteed performance. (for example from fp16 to bf16 etc.)\n",
    "\n",
    "ATLAS KG construction consist of 5 steps:\n",
    "- Triples Json Generation (Base KG Json)\n",
    "- Convert Triples Json to Triples csv\n",
    "- Conceptualize Entity in Triples csv\n",
    "- Merge Concept CSV to Triples CSV\n",
    "- Convert CSV to graphml for networkx to perform rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag import TripleGenerator, KnowledgeGraphExtractor, ProcessingConfig\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline\n",
    "# client = OpenAI(api_key='<your_api_key>',base_url=\"<your_api_base_url>\") \n",
    "# model_name = \"meta-llama/llama-3.1-8b-instruct\"\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "client = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "keyword = 'Dulce'\n",
    "output_directory = f'import/{keyword}'\n",
    "triple_generator = TripleGenerator(client, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37353f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extraction_config = ProcessingConfig(\n",
    "      model_path=model_name,\n",
    "      data_directory=\"tests\",\n",
    "      filename_pattern=keyword,\n",
    "      batch_size=2,\n",
    "      output_directory=f\"{output_directory}\",\n",
    ")\n",
    "kg_extractor = KnowledgeGraphExtractor(model=triple_generator, config=kg_extraction_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c248a3",
   "metadata": {},
   "source": [
    "### Triples Generation (with OpenAI Package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct entity&event graph\n",
    "kg_extractor.run_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Triples Json to CSV\n",
    "kg_extractor.convert_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335211ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept Generation\n",
    "kg_extractor.generate_concept_csv_temp(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5823e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extractor.create_concept_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv to graphml for networkx\n",
    "kg_extractor.convert_to_graphml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f197d",
   "metadata": {},
   "source": [
    "## ATLAS RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b957c1",
   "metadata": {},
   "source": [
    "In order to perform RAG, one need to first create embeddings & faiss index for constructed KG\n",
    "\n",
    "[There maybe performance difference in using AutoModel and Sentence Transformer for NV-Ebmed-v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6859bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs available: 2\n",
      "GPU 0: NVIDIA L20\n",
      "GPU 1: NVIDIA L20\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'  # Set to the GPU you want to use, or '0' for the first GPU\n",
    "import torch\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"number of GPUs available:\", torch.cuda.device_count())\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5d1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/httsangaj/miniconda3/envs/faiss-gpu/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.retriever import NvEmbed\n",
    "from transformers import AutoModel\n",
    "# Load the SentenceTransformer model\n",
    "encoder_model_name = \"nvidia/NV-Embed-v2\"\n",
    "# sentence_model = SentenceTransformer(encoder_model_name, trust_remote_code=True, model_kwargs={'device_map': \"auto\"})\n",
    "# sentence_model.max_seq_length = 32768\n",
    "# sentence_model.tokenizer.padding_side=\"right\"\n",
    "sentence_model = AutoModel.from_pretrained(encoder_model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "sentence_encoder = NvEmbed(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a106e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from atlas_rag.reader import LLMGenerator\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# reader_model_name = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "reader_model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "client = OpenAI(\n",
    "  # base_url=\"https://openrouter.ai/api/v1\",\n",
    "  # api_key=config['settings']['OPENROUTER_API_KEY'],\n",
    "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "  api_key=config['settings']['DEEPINFRA_API_KEY'],\n",
    ")\n",
    "llm_generator = LLMGenerator(client=client, model_name=reader_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ba40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using encoder model: NV-Embed-v2\n",
      "Loading graph from /data/httsangaj/atomic-rag/8b/kg_graphml/musique_graph.graphml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262675/262675 [00:00<00:00, 2206089.62it/s]\n",
      "100%|██████████| 262675/262675 [00:00<00:00, 1805028.41it/s]\n",
      "955769it [00:00, 3614781.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts:   0%|          | 0/1 [00:00<?, ?it/s]/home/httsangaj/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/c50d55f43bde7e6a18e0eaa15a62fd63a930f1a1/modeling_nvembed.py:349: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_dict.get('input_ids').to(batch_dict.get('input_ids')).long()),\n",
      "Encoding texts: 100%|██████████| 1/1 [00:04<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m keyword = \u001b[33m'\u001b[39m\u001b[33mmusique\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m working_directory = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/data/httsangaj/atomic-rag/8b\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = \u001b[43mcreate_embeddings_and_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentence_encoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentence_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnvidia/NV-Embed-v2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworking_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworking_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_concept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_events\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AutoSchemaKG/atlas_rag/retriever/indexer.py:153\u001b[39m, in \u001b[36mcreate_embeddings_and_index\u001b[39m\u001b[34m(sentence_encoder, model_name, working_directory, keyword, include_events, include_concept, normalize_embeddings, batch_size)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mComputing text embeddings...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    152\u001b[39m text_embeddings = compute_text_embeddings(original_text_list, sentence_encoder, batch_size, normalize_embeddings)  \u001b[38;5;66;03m# Assumes this function is defined\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m text_faiss_index = \u001b[43mbuild_faiss_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assumes this function is defined\u001b[39;00m\n\u001b[32m    154\u001b[39m faiss.write_index(text_faiss_index, text_index_path)\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(text_embeddings_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AutoSchemaKG/atlas_rag/retriever/indexer.py:69\u001b[39m, in \u001b[36mbuild_faiss_index\u001b[39m\u001b[34m(embededdings)\u001b[39m\n\u001b[32m     66\u001b[39m dimension = \u001b[38;5;28mlen\u001b[39m(embededdings[\u001b[32m0\u001b[39m])\n\u001b[32m     68\u001b[39m faiss_index = faiss.IndexHNSWFlat(dimension, \u001b[32m64\u001b[39m, faiss.METRIC_INNER_PRODUCT)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43membededdings\u001b[49m\u001b[43m)\u001b[49m.astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# normalize the vectors\u001b[39;00m\n\u001b[32m     72\u001b[39m faiss.normalize_L2(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/faiss-gpu/lib/python3.12/site-packages/torch/_tensor.py:1062\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "from atlas_rag import create_embeddings_and_index\n",
    "keyword = 'musique'\n",
    "working_directory = f'/data/httsangaj/atomic-rag/8b'\n",
    "data = create_embeddings_and_index(\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    model_name = 'nvidia/NV-Embed-v2',\n",
    "    working_directory=working_directory,\n",
    "    keyword=keyword,\n",
    "    include_concept=True,\n",
    "    include_events=True,\n",
    "    normalize_embeddings= True,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag.evaluation import BenchMarkConfig\n",
    "benchmark_config = BenchMarkConfig(\n",
    "    dataset_name= 'musique',\n",
    "    question_file= \"benchmark_data/musique.json\",\n",
    "    include_concept=True,\n",
    "    include_events=True,\n",
    "    reader_model_name=reader_model_name,\n",
    "    encoder_model_name=encoder_model_name,\n",
    "    number_of_samples=-1, # -1 for all samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag import setup_logger\n",
    "logger = setup_logger(benchmark_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize desired RAG method for benchmarking\n",
    "from atlas_rag.retriever import HippoRAG2Retriever\n",
    "hipporag2_retriever = HippoRAG2Retriever(\n",
    "    llm_generator=llm_generator,\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    data = data,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d653a",
   "metadata": {},
   "source": [
    "## Investigation for reason to perfomance difference:\n",
    "- Version difference for cuda?\n",
    "- Version difference for huggingface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b515ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start benchmarking\n",
    "from atlas_rag.evaluation import RAGBenchmark\n",
    "benchmark = RAGBenchmark(config=benchmark_config, logger=logger)\n",
    "benchmark.run([hipporag2_retriever], llm_generator=llm_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
